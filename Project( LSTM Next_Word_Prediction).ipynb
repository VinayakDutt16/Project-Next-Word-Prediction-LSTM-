{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZmWj70e42bl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Qac4MqNT5_mh",
        "outputId": "b4acaf9a-5ce0-4e9e-b34c-6dfaf3989878"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9585fa54-6724-4c01-88a9-4e598fa7ec07\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9585fa54-6724-4c01-88a9-4e598fa7ec07\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving sherlock-holm.es_stories_plain-text_advs.txt to sherlock-holm.es_stories_plain-text_advs.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "path_to_file = list(files.upload().keys())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlNyzx126LOj"
      },
      "outputs": [],
      "source": [
        "#Read the file\n",
        "with open(path_to_file, 'r',encoding = 'utf-8') as file:\n",
        "  text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntZb75ax6nSc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtDtjvxC6rJE"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgW8qlH57XTd"
      },
      "outputs": [],
      "source": [
        "# Now let's tokenize the text to create a sequence of words:\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code , the text is tokenized , which means it is divided into individual word or tokens. The 'Tokenizer' object is created ,\n",
        " which will handle the tokenization process. The 'fit_on_texts' methods of the tokenizer is called, passing the 'text' as input .\n",
        " This method analyzes the text and builds  a vocabulary of unique words , assigning each word a numerical index. The 'total_words' variable is then assigned the value of the length of the word index plus one ,\n",
        " representing the total number of distinct words in the text."
      ],
      "metadata": {
        "id": "_qkv2n96zVBn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6nLMAjN9Al8"
      },
      "outputs": [],
      "source": [
        "# Now let's create input-output pairs by spltting the text into sequences of tokens and forming n-grams from the sequences:\n",
        "input_sequences = []\n",
        "for line in text.split('\\n'):\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "In the above code, the text data is split inti lines using the '\\n' character as a delimiter. For each line in the text, the 'texts_to_sequences' method of the tokenizer is used to convert the line into a sequence of numerical tokens based on the previously created vocabulary. The resulting token list is then iterated over using a for loop. For each iteration , a subsequence, or n-gram, of tokens is extracted , ranging from the beginning of the token list up to the current index 'i'.\n",
        "\n",
        "This n-gram sequence represents the input context, with the last token being the target or predicted word. This n-gram sequence is then appended to the \"input_sequences\" list. This process is repeated for all lines in the text, generating multiple input-output sequences that will be used for training the next word prediction model.\n"
      ],
      "metadata": {
        "id": "S9sZ7fy8zEkw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRKjee0o-jQ1"
      },
      "outputs": [],
      "source": [
        "# Now let's pad the input sequences to have equal length:\n",
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen = max_sequence_len, padding = 'pre'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the above code, the input sequences are padded to ensure all sequences have the same length.\n",
        " The variable 'max_sequence_len' is assignwd the maximum length among all the input sequences.\n",
        " The 'pad_sequences' function is used to pad or truncate the input sequences to match this maximum length.\n",
        "\n",
        "The 'pad_sequences' function takes the input_sequences list, sets the maximum length to 'max_sequence_len', and specifies that the padding should be added at the beginning of each sequence using the 'padding=pre' argument. Fnally, the input sequences are converted into a numpy array to facilliate further processing.\n",
        "Now let's split the sequences into input and output:"
      ],
      "metadata": {
        "id": "E8Xueb2Fy5El"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KavAVm_AJFx"
      },
      "outputs": [],
      "source": [
        "X = input_sequences[:,:-1]\n",
        "y = input_sequences[:,-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxybbvn34fd4"
      },
      "source": [
        "In the above code, the input sequences are split into two arrays, 'X' and 'y', to create the input and output for training the next word prediction model.\n",
        "The 'X' array is assigned the values of all in the 'input_sequences' array except for the last column.\n",
        "It means that 'X' contains all the tokens in each sequence except for the last one , representing the input context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxsBlPpY4mRD"
      },
      "source": [
        "Now let's convert the output to one-hot encode vectors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk5z97DWAoK0"
      },
      "outputs": [],
      "source": [
        "y = np.array(tf.keras.utils.to_categorical(y, num_classes = total_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvBa6OnG36ku"
      },
      "source": [
        "In the above code, we are converting the output array into a suitable format for training a model, where each target word is represented as a binary vector.\n",
        "\n",
        "Now let's build a neural network architecture to train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAxsjgNAA1fp",
        "outputId": "9c70be0f-b7af-45fd-e326-423ceaac48eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 17, 100)           820000    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 150)               150600    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8200)              1238200   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2208800 (8.43 MB)\n",
            "Trainable params: 2208800 (8.43 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length = max_sequence_len-1))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words, activation = 'softmax'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70bkw3FQ5ADq"
      },
      "source": [
        "(1). 'total_words', which represents the total number of distinct words in the vocabulary;\n",
        "\n",
        "(2). '100', which denotes the dimensionality of the word embediings;\n",
        "\n",
        "(3). and 'input_lengths', which specifies the length of the input sequences.\n",
        "\n",
        "The next layer added is the 'LSTM' layer , a type of recurrent neural network (RNN) layer designed for capturing sequential dependencies in the data. It has 150 units, which means it will learn 150 internal representations or memory cells.\n",
        "\n",
        "Finally, the 'Dense' layer is added, which is a fully connected layer that produces the output predictions. It has 'total_words' units and uses the 'softmax' activation function to convert the predicted scores into probabilities, indicating the likelihood of each word being the next one in the sequence.\n",
        "\n",
        "Now let's compile and train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYassizeDLTk",
        "outputId": "3fe321b1-37be-4642-c431-6d80d40d996b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "3010/3010 [==============================] - 270s 89ms/step - loss: 6.2433 - accuracy: 0.0759\n",
            "Epoch 2/100\n",
            "3010/3010 [==============================] - 232s 77ms/step - loss: 5.5218 - accuracy: 0.1231\n",
            "Epoch 3/100\n",
            "3010/3010 [==============================] - 250s 83ms/step - loss: 5.1245 - accuracy: 0.1475\n",
            "Epoch 4/100\n",
            "3010/3010 [==============================] - 219s 73ms/step - loss: 4.7911 - accuracy: 0.1652\n",
            "Epoch 5/100\n",
            "3010/3010 [==============================] - 219s 73ms/step - loss: 4.4854 - accuracy: 0.1823\n",
            "Epoch 6/100\n",
            "3010/3010 [==============================] - 218s 73ms/step - loss: 4.1993 - accuracy: 0.2017\n",
            "Epoch 7/100\n",
            "3010/3010 [==============================] - 219s 73ms/step - loss: 3.9301 - accuracy: 0.2267\n",
            "Epoch 8/100\n",
            "3010/3010 [==============================] - 217s 72ms/step - loss: 3.6753 - accuracy: 0.2547\n",
            "Epoch 9/100\n",
            "3010/3010 [==============================] - 220s 73ms/step - loss: 3.4322 - accuracy: 0.2897\n",
            "Epoch 10/100\n",
            "3010/3010 [==============================] - 220s 73ms/step - loss: 3.2081 - accuracy: 0.3234\n",
            "Epoch 11/100\n",
            "3010/3010 [==============================] - 226s 75ms/step - loss: 2.9989 - accuracy: 0.3575\n",
            "Epoch 12/100\n",
            "3010/3010 [==============================] - 221s 73ms/step - loss: 2.8080 - accuracy: 0.3913\n",
            "Epoch 13/100\n",
            "3010/3010 [==============================] - 218s 72ms/step - loss: 2.6310 - accuracy: 0.4254\n",
            "Epoch 14/100\n",
            "3010/3010 [==============================] - 220s 73ms/step - loss: 2.4674 - accuracy: 0.4556\n",
            "Epoch 15/100\n",
            "3010/3010 [==============================] - 216s 72ms/step - loss: 2.3173 - accuracy: 0.4866\n",
            "Epoch 16/100\n",
            "3010/3010 [==============================] - 218s 72ms/step - loss: 2.1797 - accuracy: 0.5153\n",
            "Epoch 17/100\n",
            "3010/3010 [==============================] - 216s 72ms/step - loss: 2.0522 - accuracy: 0.5399\n",
            "Epoch 18/100\n",
            "3010/3010 [==============================] - 221s 73ms/step - loss: 1.9363 - accuracy: 0.5635\n",
            "Epoch 19/100\n",
            "3010/3010 [==============================] - 219s 73ms/step - loss: 1.8305 - accuracy: 0.5886\n",
            "Epoch 20/100\n",
            "3010/3010 [==============================] - 220s 73ms/step - loss: 1.7300 - accuracy: 0.6101\n",
            "Epoch 21/100\n",
            "3010/3010 [==============================] - 227s 75ms/step - loss: 1.6410 - accuracy: 0.6284\n",
            "Epoch 22/100\n",
            "3010/3010 [==============================] - 228s 76ms/step - loss: 1.5563 - accuracy: 0.6483\n",
            "Epoch 23/100\n",
            "3010/3010 [==============================] - 231s 77ms/step - loss: 1.4802 - accuracy: 0.6635\n",
            "Epoch 24/100\n",
            "3010/3010 [==============================] - 224s 74ms/step - loss: 1.4111 - accuracy: 0.6791\n",
            "Epoch 25/100\n",
            "3010/3010 [==============================] - 221s 73ms/step - loss: 1.3444 - accuracy: 0.6940\n",
            "Epoch 26/100\n",
            "3010/3010 [==============================] - 219s 73ms/step - loss: 1.2836 - accuracy: 0.7075\n",
            "Epoch 27/100\n",
            "3010/3010 [==============================] - 222s 74ms/step - loss: 1.2305 - accuracy: 0.7182\n",
            "Epoch 28/100\n",
            "3010/3010 [==============================] - 222s 74ms/step - loss: 1.1784 - accuracy: 0.7307\n",
            "Epoch 29/100\n",
            "3010/3010 [==============================] - 224s 75ms/step - loss: 1.1304 - accuracy: 0.7410\n",
            "Epoch 30/100\n",
            "3010/3010 [==============================] - 219s 73ms/step - loss: 1.0875 - accuracy: 0.7504\n",
            "Epoch 31/100\n",
            "3010/3010 [==============================] - 218s 72ms/step - loss: 1.0483 - accuracy: 0.7592\n",
            "Epoch 32/100\n",
            "3010/3010 [==============================] - 219s 73ms/step - loss: 1.0083 - accuracy: 0.7688\n",
            "Epoch 33/100\n",
            "3010/3010 [==============================] - 219s 73ms/step - loss: 0.9762 - accuracy: 0.7752\n",
            "Epoch 34/100\n",
            "3010/3010 [==============================] - 220s 73ms/step - loss: 0.9462 - accuracy: 0.7813\n",
            "Epoch 35/100\n",
            "3010/3010 [==============================] - 218s 72ms/step - loss: 0.9142 - accuracy: 0.7901\n",
            "Epoch 36/100\n",
            "3010/3010 [==============================] - 221s 73ms/step - loss: 0.8877 - accuracy: 0.7944\n",
            "Epoch 37/100\n",
            "3010/3010 [==============================] - 220s 73ms/step - loss: 0.8641 - accuracy: 0.7989\n",
            "Epoch 38/100\n",
            "3010/3010 [==============================] - 219s 73ms/step - loss: 0.8405 - accuracy: 0.8036\n",
            "Epoch 39/100\n",
            "3010/3010 [==============================] - 219s 73ms/step - loss: 0.8160 - accuracy: 0.8097\n",
            "Epoch 40/100\n",
            "3010/3010 [==============================] - 219s 73ms/step - loss: 0.7995 - accuracy: 0.8126\n",
            "Epoch 41/100\n",
            "3010/3010 [==============================] - 223s 74ms/step - loss: 0.7778 - accuracy: 0.8174\n",
            "Epoch 42/100\n",
            "3010/3010 [==============================] - 220s 73ms/step - loss: 0.7666 - accuracy: 0.8188\n",
            "Epoch 43/100\n",
            "3010/3010 [==============================] - 218s 73ms/step - loss: 0.7483 - accuracy: 0.8236\n",
            "Epoch 44/100\n",
            "3010/3010 [==============================] - 225s 75ms/step - loss: 0.7338 - accuracy: 0.8262\n",
            "Epoch 45/100\n",
            "3010/3010 [==============================] - 220s 73ms/step - loss: 0.7179 - accuracy: 0.8296\n",
            "Epoch 46/100\n",
            "3010/3010 [==============================] - 218s 72ms/step - loss: 0.7074 - accuracy: 0.8314\n",
            "Epoch 47/100\n",
            "3010/3010 [==============================] - 217s 72ms/step - loss: 0.6982 - accuracy: 0.8332\n",
            "Epoch 48/100\n",
            "3010/3010 [==============================] - 221s 74ms/step - loss: 0.6852 - accuracy: 0.8355\n",
            "Epoch 49/100\n",
            "3010/3010 [==============================] - 220s 73ms/step - loss: 0.6750 - accuracy: 0.8379\n",
            "Epoch 50/100\n",
            "3010/3010 [==============================] - 221s 73ms/step - loss: 0.6677 - accuracy: 0.8393\n",
            "Epoch 51/100\n",
            "3010/3010 [==============================] - 222s 74ms/step - loss: 0.6540 - accuracy: 0.8421\n",
            "Epoch 52/100\n",
            "3010/3010 [==============================] - 220s 73ms/step - loss: 0.6479 - accuracy: 0.8429\n",
            "Epoch 53/100\n",
            "3010/3010 [==============================] - 213s 71ms/step - loss: 0.6397 - accuracy: 0.8442\n",
            "Epoch 54/100\n",
            "3010/3010 [==============================] - 201s 67ms/step - loss: 0.6306 - accuracy: 0.8468\n",
            "Epoch 55/100\n",
            "3010/3010 [==============================] - 203s 67ms/step - loss: 0.6287 - accuracy: 0.8457\n",
            "Epoch 56/100\n",
            "3010/3010 [==============================] - 203s 67ms/step - loss: 0.6217 - accuracy: 0.8466\n",
            "Epoch 57/100\n",
            "3010/3010 [==============================] - 198s 66ms/step - loss: 0.6126 - accuracy: 0.8498\n",
            "Epoch 58/100\n",
            "3010/3010 [==============================] - 205s 68ms/step - loss: 0.6104 - accuracy: 0.8496\n",
            "Epoch 59/100\n",
            "3010/3010 [==============================] - 205s 68ms/step - loss: 0.6053 - accuracy: 0.8497\n",
            "Epoch 60/100\n",
            "3010/3010 [==============================] - 207s 69ms/step - loss: 0.5986 - accuracy: 0.8517\n",
            "Epoch 61/100\n",
            "3010/3010 [==============================] - 209s 69ms/step - loss: 0.5999 - accuracy: 0.8505\n",
            "Epoch 62/100\n",
            "3010/3010 [==============================] - 206s 69ms/step - loss: 0.5907 - accuracy: 0.8530\n",
            "Epoch 63/100\n",
            "3010/3010 [==============================] - 206s 68ms/step - loss: 0.5853 - accuracy: 0.8546\n",
            "Epoch 64/100\n",
            "3010/3010 [==============================] - 208s 69ms/step - loss: 0.5833 - accuracy: 0.8553\n",
            "Epoch 65/100\n",
            "3010/3010 [==============================] - 207s 69ms/step - loss: 0.5809 - accuracy: 0.8552\n",
            "Epoch 66/100\n",
            "2417/3010 [=======================>......] - ETA: 41s - loss: 0.5514 - accuracy: 0.8616"
          ]
        }
      ],
      "source": [
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam' , metrics = ['accuracy'])\n",
        "model.fit(X, y, epochs = 100, verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2euzNJv8Dd3_"
      },
      "outputs": [],
      "source": [
        "# seed_text = \"I will \"\n",
        "seed_text = \"Are you\"\n",
        "next_words = 3\n",
        "\n",
        "for _ in range(next_words):\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  token_list = pad_sequences([token_list], maxlen = max_sequence_len - 1, padding = 'pre')\n",
        "  predicted = np.argmax(model.predict(token_list), axis = -1)\n",
        "  output_word = \"\"\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == predicted:\n",
        "      output_word = word\n",
        "      break\n",
        "  seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXLnjaar6lYr"
      },
      "source": [
        "The above code generates the next word predictions based on a given seed text . The 'seed_text' variable holds the initial text. The 'next_words' variable detrmines the number of predictions to be generated. Inside the for loop, the 'seed_text' is converted into a sequence of tokens using the tokenizer. The token sequence is padded to match the maximum sequence length .\n",
        "\n",
        "The model predicts the next word by calling the 'predict' method on the model with the padded token sequence. The predicted word is obtained by finding the word  with the highest probability score  using 'np.argmax'. Then, the predicted word is appended ro the 'seed_text', and the process is repeated for the desired number of"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKy9FoG26mU1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}